from flask import Flask, request, Response, jsonify
from flask_cors import CORS
import uuid
import time
import socket
import logging
import json
import os
import argparse
import sys

# Import c√°c module ƒë√£ t√°ch
from utils.logging_config import setup_logging
from utils.queue_manager import queue_manager
from utils.terminal_ui import terminal_ui
from utils.chat_manager import chat_manager
from services.qwen_service import qwen_service
from services.chat_service import chat_service
from services.ollama_service import ollama_service
from models.request_state import RequestState

# Parse command line arguments
def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Custom Server with Qwen API Integration')
    parser.add_argument('--background', action='store_true', 
                       help='Run server in background mode (no terminal output)')
    parser.add_argument('--mode', choices=['lmstudio', 'ollama'], 
                       help='Server mode (lmstudio or ollama)')
    parser.add_argument('--port', type=int, 
                       help='Server port (default: 1235 for lmstudio, 11434 for ollama)')
    parser.add_argument('--host', default='0.0.0.0', 
                       help='Server host (default: 0.0.0.0)')
    return parser.parse_args()

# C·∫•u h√¨nh Flask ƒë·ªÉ tr·∫£ v·ªÅ JSON ƒë·∫πp
app = Flask(__name__)
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True

# C·∫•u h√¨nh CORS ƒë·ªÉ cho ph√©p t·∫•t c·∫£ origin
CORS(app, origins="*", supports_credentials=True)

# C·∫•u h√¨nh ƒë·ªÉ ch·∫•p nh·∫≠n header request r·∫•t d√†i
app.config['MAX_CONTENT_LENGTH'] = 2048 * 1024 * 1024  # 2GB
app.config['MAX_CONTENT_PATH'] = None
app.config['MAX_COOKIE_SIZE'] = 2048 * 1024 * 1024  # 2GB

# TƒÉng gi·ªõi h·∫°n cho request body
app.config['MAX_CONTENT_LENGTH'] = None  # Kh√¥ng gi·ªõi h·∫°n

# C·∫•u h√¨nh th√™m ƒë·ªÉ x·ª≠ l√Ω request l·ªõn
app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0
app.config['TEMPLATES_AUTO_RELOAD'] = True

# C·∫•u h√¨nh ƒë·ªÉ ch·∫•p nh·∫≠n request kh√¥ng c√≥ Content-Type
app.config['JSON_AS_ASCII'] = False
app.config['JSONIFY_MIMETYPE'] = 'application/json'

# T·∫Øt ki·ªÉm tra Content-Type cho JSON
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True
app.config['JSON_SORT_KEYS'] = False

# TƒÉng gi·ªõi h·∫°n JSON serialization
import sys
sys.setrecursionlimit(10000)  # TƒÉng recursion limit

# Global variables
SERVER_MODE = None
BACKGROUND_MODE = False
args = None

# Parse arguments first
args = parse_arguments()

# Setup logging based on background mode
def setup_logging_with_background():
    """Setup logging based on background mode"""
    global BACKGROUND_MODE
    if args.background:
        BACKGROUND_MODE = True
        # Redirect all output to null in background mode
        import os
        import sys
        
        # Redirect stdout and stderr to null
        if os.name == 'nt':  # Windows
            null_device = 'NUL'
        else:  # Unix/Linux/Mac
            null_device = '/dev/null'
        
        # Open null device
        null_fd = os.open(null_device, os.O_RDWR)
        
        # Redirect stdout and stderr
        os.dup2(null_fd, sys.stdout.fileno())
        os.dup2(null_fd, sys.stderr.fileno())
        
        # Close the null device
        os.close(null_fd)
        
        # Setup minimal logging for background mode
        logging.basicConfig(
            level=logging.ERROR,  # Only log errors
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.devnull, mode='w'),  # Log to null
                logging.NullHandler()  # No console output
            ]
        )
        return logging.getLogger(__name__)
    else:
        # Normal logging setup
        return setup_logging()

logger = setup_logging_with_background()

# Override ƒë·ªÉ b·ªè qua ki·ªÉm tra Content-Type
@app.before_request
def before_request():
    """Override ƒë·ªÉ x·ª≠ l√Ω request kh√¥ng c√≥ Content-Type"""
    if request.method == 'POST' and request.path.startswith('/api/'):
        # N·∫øu l√† POST request ƒë·∫øn Ollama API v√† kh√¥ng c√≥ Content-Type
        if not request.content_type or 'application/json' not in request.content_type:
            # Set Content-Type ƒë·ªÉ Flask kh√¥ng b√°o l·ªói
            request.environ['CONTENT_TYPE'] = 'application/json'

@app.route('/', methods=['GET'])
def root():
    """Root endpoint"""
    route_info = "GET / - Root"
    terminal_ui.update_route(route_info)
    
    return "Ollama is running"

@app.route('/', methods=['OPTIONS'])
def root_options():
    """OPTIONS for root endpoint"""
    route_info = "OPTIONS / - Root"
    terminal_ui.update_route(route_info)
    
    response = Response()
    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
    response.headers['Access-Control-Allow-Headers'] = 'Content-Type'
    return response

def parse_tools_to_text(tools):
    """Parse tools th√†nh text format"""
    tools_text = ""
    for i, tool in enumerate(tools):
        if tool.get('type') == 'function':
            func = tool.get('function', {})
            name = func.get('name', '')
            description = func.get('description', '')
            parameters = func.get('parameters', {})
            
            tools_text += f"Function {i+1}: {name}\n"
            if description:
                tools_text += f"Description: {description}\n"
            
            if parameters:
                param_type = parameters.get('type', 'object')
                tools_text += f"Parameters (type: {param_type}):\n"
                
                props = parameters.get('properties', {})
                required = parameters.get('required', [])
                
                for prop_name, prop_info in props.items():
                    prop_type = prop_info.get('type', 'string')
                    prop_desc = prop_info.get('description', '')
                    is_required = prop_name in required
                    
                    tools_text += f"  - {prop_name} ({prop_type})"
                    if is_required:
                        tools_text += " [required]"
                    if prop_desc:
                        tools_text += f": {prop_desc}"
                    tools_text += "\n"
                    
                    # Handle enum values
                    if 'enum' in prop_info:
                        enum_values = prop_info['enum']
                        tools_text += f"    Values: {', '.join(enum_values)}\n"
            
            tools_text += "\n"
    
    return tools_text

def parse_json_request():
    """Decorator ƒë·ªÉ parse JSON request kh√¥ng c·∫ßn Content-Type"""
    def decorator(f):
        import functools
        @functools.wraps(f)
        def wrapper(*args, **kwargs):
            try:
                if request.content_type and 'application/json' in request.content_type:
                    # N·∫øu c√≥ Content-Type application/json, d√πng get_json()
                    request.json_data = request.get_json()
                else:
                    # N·∫øu kh√¥ng c√≥ Content-Type, parse t·ª´ raw data
                    raw_data = request.get_data(as_text=True)
                    if raw_data:
                        request.json_data = json.loads(raw_data)
                    else:
                        request.json_data = {}
            except Exception as e:
                return jsonify({"error": f"Invalid JSON: {str(e)}"}), 400
            return f(*args, **kwargs)
        return wrapper
    return decorator

def ask_server_mode():
    """H·ªèi ng∆∞·ªùi d√πng ch·ªçn mode server ho·∫∑c s·ª≠ d·ª•ng argument"""
    global SERVER_MODE
    
    # N·∫øu c√≥ argument --mode, s·ª≠ d·ª•ng lu√¥n
    if args.mode:
        SERVER_MODE = args.mode
        if SERVER_MODE == "lmstudio":
            if not BACKGROUND_MODE:
                print("‚úÖ ƒê√£ ch·ªçn LM Studio Mode - Port 1235")
            return 1235
        else:
            if not BACKGROUND_MODE:
                print("‚úÖ ƒê√£ ch·ªçn Ollama Mode - Port 11434")
            return 11434
    
    # N·∫øu c√≥ argument --port, x√°c ƒë·ªãnh mode d·ª±a tr√™n port
    if args.port:
        if args.port == 1235:
            SERVER_MODE = "lmstudio"
            if not BACKGROUND_MODE:
                print("‚úÖ ƒê√£ ch·ªçn LM Studio Mode - Port 1235")
            return 1235
        elif args.port == 11434:
            SERVER_MODE = "ollama"
            if not BACKGROUND_MODE:
                print("‚úÖ ƒê√£ ch·ªçn Ollama Mode - Port 11434")
            return 11434
        else:
            # Port t√πy ch·ªânh, h·ªèi mode
            pass
    
    # Trong background mode, default to lmstudio n·∫øu kh√¥ng c√≥ argument
    if BACKGROUND_MODE:
        SERVER_MODE = "lmstudio"
        return 1235
    
    # H·ªèi ng∆∞·ªùi d√πng ch·ªçn mode n·∫øu kh√¥ng c√≥ argument
    print("\n" + "="*50)
    print("ü§ñ CUSTOM SERVER - CH·ªåN MODE")
    print("="*50)
    print("1. LM Studio Mode (port 1235)")
    print("2. Ollama Mode (port 11434)")
    print("="*50)
    
    while True:
        try:
            choice = input("Ch·ªçn mode (1 ho·∫∑c 2): ").strip()
            if choice == "1":
                SERVER_MODE = "lmstudio"
                print("‚úÖ ƒê√£ ch·ªçn LM Studio Mode - Port 1235")
                return 1235
            elif choice == "2":
                SERVER_MODE = "ollama"
                print("‚úÖ ƒê√£ ch·ªçn Ollama Mode - Port 11434")
                return 11434
            else:
                print("‚ùå Vui l√≤ng ch·ªçn 1 ho·∫∑c 2")
        except KeyboardInterrupt:
            print("\nüõë Tho√°t ch∆∞∆°ng tr√¨nh")
            sys.exit(0)

# LM Studio API Endpoints
@app.route('/v1/models', methods=['GET', 'OPTIONS'])
def list_models():
    """List the currently loaded models"""
    if request.method == 'OPTIONS':
        route_info = "OPTIONS /v1/models - List Models"
        terminal_ui.update_route(route_info)
        
        response = Response()
        response.headers['Access-Control-Allow-Origin'] = '*'
        response.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type'
        return response
    
    route_info = "GET /v1/models - List Models"
    terminal_ui.update_route(route_info)
    
    models = qwen_service.get_models_from_qwen()
    
    if SERVER_MODE == "ollama":
        # Convert to Ollama format
        formatted_models = []
        for model in models:
            model_id = model.get('id', '')
            if model_id:
                # Add :latest suffix for Ollama format
                model_name_with_latest = f"{model_id}:latest"
                formatted_models.append({
                    "id": model_name_with_latest,
                    "object": "model",
                    "created": int(time.time()),
                    "owned_by": "library"
                })
    else:
        # LM Studio format - use original models data
        formatted_models = models
    
    response = jsonify({
        "object": "list",
        "data": formatted_models
    })
    response.headers['Content-Type'] = 'application/json'
    return response

@app.route('/v1/models/<model_id>', methods=['GET'])
def get_model(model_id):
    """Get specific model information"""
    if SERVER_MODE != "lmstudio":
        return jsonify({"error": "Endpoint not available in current mode"}), 404
        
    route_info = f"GET /v1/models/{model_id} - Get Model Info"
    terminal_ui.update_route(route_info)
    
    try:
        # L·∫•y th√¥ng tin model t·ª´ Qwen API
        qwen_models = qwen_service.get_models_from_qwen()
        
        # T√¨m model c·ª• th·ªÉ
        target_model = None
        for model in qwen_models:
            if model.get('id') == model_id:
                target_model = model
                break
        
        if not target_model:
            # N·∫øu kh√¥ng t√¨m th·∫•y, tr·∫£ v·ªÅ l·ªói
            return jsonify({
                "error": {
                    "message": f"Model {model_id} not found",
                    "type": "invalid_request_error",
                    "code": "model_not_found"
                }
            }), 404
        
        # L·∫•y th√¥ng tin chi ti·∫øt t·ª´ info.meta
        model_info = target_model.get('info', {})
        meta = model_info.get('meta', {})
        capabilities = meta.get('capabilities', {})
        
        # Map th√¥ng tin t·ª´ Qwen API sang LM Studio format
        context_window = meta.get('max_context_length', 131072)
        
        # X√°c ƒë·ªãnh reservedOutputTokenSpace
        if 'max_thinking_generation_length' in meta:
            reserved_output_space = meta.get('max_thinking_generation_length')
        elif 'max_summary_generation_length' in meta:
            reserved_output_space = meta.get('max_summary_generation_length')
        elif 'max_generation_length' in meta:
            reserved_output_space = meta.get('max_generation_length')
        else:
            reserved_output_space = 8192
        
        # Ki·ªÉm tra c√≥ h·ªó tr·ª£ thinking kh√¥ng
        supports_thinking = capabilities.get('thinking', False) or capabilities.get('thinking_budget', False)
        
        # Map capabilities
        lm_capabilities = {
            "vision": capabilities.get('vision', False),
            "function_calling": True,  # Qwen h·ªó tr·ª£ function calling
            "json_output": True,       # Qwen h·ªó tr·ª£ JSON output
            "streaming": True,         # Qwen h·ªó tr·ª£ streaming
            "document": capabilities.get('document', False),
            "video": capabilities.get('video', False),
            "audio": capabilities.get('audio', False),
            "citations": capabilities.get('citations', False)
        }
        
        # T·∫°o response cho LM Studio
        model_config = {
            "id": model_id,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "qwen",
            "permission": [],
            "root": model_id,
            "parent": None,
            "contextWindow": context_window,
            "reservedOutputTokenSpace": reserved_output_space,
            "supportsSystemMessage": "system-role",
            "reasoningCapabilities": {
                "supportsReasoning": supports_thinking,
                "canTurnOffReasoning": supports_thinking,
                "canIOReasoning": supports_thinking,
                "openSourceThinkTags": [
                    "<think>",
                    "</think>"
                ] if supports_thinking else []
            },
            "capabilities": lm_capabilities,
            "pricing": {
                "prompt": 0.0001,
                "completion": 0.0002
            }
        }
                
        response = jsonify(model_config)
        response.headers['Content-Type'] = 'application/json'
        return response
        
    except Exception as e:
        logger.error(f"Error getting model info for {model_id}: {e}")
        return jsonify({
            "error": {
                "message": f"Failed to get model information: {str(e)}",
                "type": "server_error"
            }
        }), 500

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    """Chat completions with streaming support"""
    data = request.get_json()
    stream = data.get('stream', False)
    model = data.get('model', 'qwen3-235b-a22b')
    
    # Remove :latest suffix for Ollama mode
    if SERVER_MODE == "ollama" and model.endswith(':latest'):
        model = model[:-7]  # Remove :latest
        data['model'] = model  # Update the model in data
    
    route_info = f"POST /v1/chat/completions - Chat ({model}, stream: {stream})"
    terminal_ui.update_route(route_info)
    
    if stream:
        return Response(
            stream_qwen_response_with_queue(data),
            mimetype='text/plain',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Content-Type': 'text/event-stream'
            }
        )
    else:
        # Non-streaming response - proxy to Qwen API
        return stream_qwen_response_non_streaming_with_queue(data)

# Ollama API Endpoints
@app.route('/api/tags', methods=['GET'])
def ollama_list_models():
    """Ollama API: List models"""
    if SERVER_MODE != "ollama":
        return jsonify({"error": "Endpoint not available in current mode"}), 404
        
    route_info = "GET /api/tags - Ollama List Models"
    terminal_ui.update_route(route_info)
    
    try:
        qwen_models = qwen_service.get_models_from_qwen()
        
        # Convert Qwen models to Ollama format
        ollama_models = []
        for model in qwen_models:
            model_id = model.get('id', '')
            if model_id:
                # Format timestamp nh∆∞ Ollama
                from datetime import datetime
                modified_at = datetime.now().isoformat() + "+07:00"
                
                # Th√™m suffix :latest cho model name
                model_name_with_latest = f"{model_id}:latest"
                
                ollama_models.append({
                    "name": model_name_with_latest,
                    "model": model_name_with_latest,
                    "modified_at": modified_at,
                    "size": 4661224676,  # Default size
                    "digest": "365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1",  # Default digest
                    "details": {
                        "parent_model": "",
                        "format": "gguf",
                        "family": "qwen",
                        "families": ["qwen"],
                        "parameter_size": "235B",
                        "quantization_level": "Q4_0"
                    }
                })
        
        return jsonify({"models": ollama_models})
        
    except Exception as e:
        logger.error(f"Error listing Ollama models: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/version', methods=['GET'])
def ollama_version():
    """Ollama API: Get version"""
    if SERVER_MODE != "ollama":
        return jsonify({"error": "Endpoint not available in current mode"}), 404
        
    route_info = "GET /api/version - Ollama Version"
    terminal_ui.update_route(route_info)
    
    return jsonify({"version": "0.5.11"})

@app.route('/api/ps', methods=['GET'])
def ollama_list_running_models():
    """Ollama API: List running models"""
    if SERVER_MODE != "ollama":
        return jsonify({"error": "Endpoint not available in current mode"}), 404
        
    route_info = "GET /api/ps - Ollama List Running Models"
    terminal_ui.update_route(route_info)
    
    try:
        # L·∫•y t·∫•t c·∫£ models t·ª´ Qwen API (v√¨ t·∫•t c·∫£ ƒë·ªÅu ƒëang ch·∫°y theo m·∫∑c ƒë·ªãnh)
        qwen_models = qwen_service.get_models_from_qwen()
        
        # Convert Qwen models to Ollama running format
        from datetime import datetime, timedelta
        
        running_models = []
        for model in qwen_models:
            model_id = model.get('id', '')
            if model_id:
                # T√≠nh th·ªùi gian h·∫øt h·∫°n (30 ph√∫t t·ª´ b√¢y gi·ªù)
                expires_at = (datetime.now() + timedelta(minutes=30)).isoformat() + "+07:00"
                
                # Th√™m suffix :latest cho model name
                model_name_with_latest = f"{model_id}:latest"
                
                running_models.append({
                    "name": model_name_with_latest,
                    "model": model_name_with_latest,
                    "size": 6654289920,
                    "digest": "365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1",
                    "details": {
                        "parent_model": "",
                        "format": "gguf",
                        "family": "qwen",
                        "families": ["qwen"],
                        "parameter_size": "235B",
                        "quantization_level": "Q4_0"
                    },
                    "expires_at": expires_at,
                    "size_vram": 6654289920
                })
        
        return jsonify({"models": running_models})
        
    except Exception as e:
        logger.error(f"Error listing running Ollama models: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/show', methods=['POST'])
@parse_json_request()
def ollama_show_model():
    """Ollama API: Show model details"""
    if SERVER_MODE != "ollama":
        return jsonify({"error": "Endpoint not available in current mode"}), 404
        
    data = request.json_data
    model_name = data.get('name', '')
    
    route_info = f"POST /api/show - Ollama Show Model ({model_name})"
    terminal_ui.update_route(route_info)
    
    try:
        qwen_models = qwen_service.get_models_from_qwen()
        
        # Find the specific model
        target_model = None
        for model in qwen_models:
            if model.get('id') == model_name:
                target_model = model
                break
        
        if not target_model:
            return jsonify({"error": f"Model {model_name} not found"}), 404
        
        # Convert to Ollama format
        model_info = target_model.get('info', {})
        meta = model_info.get('meta', {})
        
        ollama_model_info = {
            "license": "Apache 2.0",
            "modelfile": f"FROM {model_name}",
            "parameters": str(meta.get('max_context_length', 131072)),
            "template": "{{ .Prompt }}",
            "system": "",
            "details": {
                "format": "gguf",
                "family": "qwen",
                "families": ["qwen"],
                "parameter_size": str(meta.get('max_context_length', 131072)),
                "quantization_level": "q4_0"
            }
        }
        
        return jsonify(ollama_model_info)
        
    except Exception as e:
        logger.error(f"Error showing Ollama model {model_name}: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/generate', methods=['POST'])
@parse_json_request()
def ollama_generate():
    """Ollama API: Generate response"""
    if SERVER_MODE != "ollama":
        return jsonify({"error": "Endpoint not available in current mode"}), 404
        
    data = request.json_data
    model = data.get('model', 'qwen3-235b-a22b')
    prompt = data.get('prompt', '')
    stream = data.get('stream', False)
    
    # X·ª≠ l√Ω model name c√≥ suffix :latest
    if model.endswith(':latest'):
        model = model[:-7]  # B·ªè :latest
    
    route_info = f"POST /api/generate - Ollama Generate ({model}, stream: {stream})"
    terminal_ui.update_route(route_info)
    
    # Convert Ollama format to OpenAI format
    openai_data = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": stream,
        "temperature": data.get('temperature', 0.7),
        "top_p": data.get('top_p', 1.0),
        "max_tokens": data.get('num_predict', 1000)
    }
    
    if stream:
        return Response(
            ollama_service.stream_ollama_response(openai_data),
            mimetype='application/json',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive'
            }
        )
    else:
        return ollama_service.stream_ollama_response_non_streaming(openai_data)

@app.route('/api/chat', methods=['POST'])
@parse_json_request()
def ollama_chat():
    """Ollama API: Chat endpoint"""
    if SERVER_MODE != "ollama":
        return jsonify({"error": "Endpoint not available in current mode"}), 404
        
    data = request.json_data
    model = data.get('model', 'qwen3-235b-a22b')
    messages = data.get('messages', [])
    # Default stream l√† True, ch·ªâ khi c√≥ "stream": false th√¨ m·ªõi l√† non-streaming
    stream = data.get('stream', True)
    tools = data.get('tools', [])
    
    # X·ª≠ l√Ω model name c√≥ suffix :latest
    if model.endswith(':latest'):
        model = model[:-7]  # B·ªè :latest
    
    route_info = f"POST /api/chat - Ollama Chat ({model}, stream: {stream})"
    terminal_ui.update_route(route_info)
    
    # Parse tools th√†nh text n·∫øu c√≥
    if tools:
        tools_text = parse_tools_to_text(tools)
        # Th√™m tools text v√†o message cu·ªëi c√πng
        if messages:
            last_message = messages[-1]
            if last_message.get('role') == 'user':
                last_message['content'] = f"{last_message['content']}\n\nAvailable tools:\n{tools_text}"
    
    # Convert Ollama chat format to OpenAI format
    openai_data = {
        "model": model,
        "messages": messages,
        "stream": stream,
        "temperature": data.get('temperature', 0.7),
        "top_p": data.get('top_p', 1.0),
        "max_tokens": data.get('num_predict', 1000)
    }
    
    if stream:
        return Response(
            ollama_service.stream_ollama_response(openai_data),
            mimetype='application/json',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive'
            }
        )
    else:
        return ollama_service.stream_ollama_response_non_streaming(openai_data)

def stream_qwen_response_with_queue(data):
    """Stream response from Qwen API with queue system"""
    model = data.get('model', 'qwen3-235b-a22b')
    request_id = str(uuid.uuid4())
        
    # ƒê·ª£i cho ƒë·∫øn khi c√≥ th·ªÉ x·ª≠ l√Ω v·ªõi timeout
    if not queue_manager.acquire_lock(request_id):
        yield f"data: {json.dumps({'error': 'Server busy, request timed out'})}\n\n"
        return
    
    try:
        # T·∫°o request state cho request n√†y
        request_state = RequestState(request_id, model)
        
        # X·ª≠ l√Ω request hi·ªán t·∫°i
        for chunk in chat_service.stream_qwen_response(data, request_state):
            yield chunk
    
    except Exception as e:
        logger.error(f"Error in stream_qwen_response_with_queue: {e}")
        yield f"data: {json.dumps({'error': f'Stream error: {str(e)}'})}\n\n"
    finally:
        # ƒê·∫£m b·∫£o lock ƒë∆∞·ª£c release
        queue_manager.release_lock(request_id)

def stream_qwen_response_non_streaming_with_queue(data):
    """Non-streaming response from Qwen API with queue system"""
    model = data.get('model', 'qwen3-235b-a22b')
    request_id = str(uuid.uuid4())
        
    # Ki·ªÉm tra n·∫øu c√≥ request ƒëang x·ª≠ l√Ω
    with queue_manager.chat_lock:
        if queue_manager.current_processing:
            return jsonify({
                "error": {
                    "message": "Server busy, please try again later",
                    "type": "server_error",
                    "code": "server_busy"
                }
            }), 503
        else:
            queue_manager.current_processing = True
            queue_manager.current_processing_start_time = time.time()
    
    try:
        # X·ª≠ l√Ω request hi·ªán t·∫°i
        result = chat_service.stream_qwen_response_non_streaming(data)
        
        # X·ª≠ l√Ω queue sau khi ho√†n th√†nh - ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω trong finally block
        
        return result
    
    except Exception as e:
        logger.error(f"Error in stream_qwen_response_non_streaming_with_queue: {e}")
        return jsonify({
            "error": {
                "message": f"Stream error: {str(e)}",
                "type": "server_error"
            }
        }), 500
    finally:
        # ƒê·∫£m b·∫£o lock ƒë∆∞·ª£c release
        queue_manager.release_lock(request_id)



@app.errorhandler(500)
def internal_error(error):
    """Handle internal server errors and release lock"""
    route_info = "ERROR 500 - Internal Server Error"
    logger.error(f"ROUTE: {route_info}")
    terminal_ui.update_route(route_info)
    
    logger.error(f"Internal server error: {error}")
    queue_manager.release_lock("error_handler")
    return jsonify({
        "error": {
            "message": "Internal server error",
            "type": "server_error"
        }
    }), 500

@app.route('/v1/completions', methods=['POST'])
def completions():
    """Text completions (deprecated)"""
    route_info = "POST /v1/completions - Deprecated"
    terminal_ui.update_route(route_info)
    
    return jsonify({
        "error": {
            "message": "This endpoint is deprecated. Use /v1/chat/completions instead.",
            "type": "invalid_request_error",
            "code": "deprecated_endpoint"
        }
    }), 400

@app.route('/v1/embeddings', methods=['POST'])
def embeddings():
    """Text embeddings"""
    route_info = "POST /v1/embeddings - Not Supported"
    terminal_ui.update_route(route_info)
    
    return jsonify({
        "error": {
            "message": "Embeddings not supported in this server",
            "type": "invalid_request_error",
            "code": "not_supported"
        }
    }), 400

if __name__ == '__main__':
    # H·ªèi mode tr∆∞·ªõc khi start
    port = ask_server_mode()
    
    # S·ª≠ d·ª•ng port t·ª´ argument n·∫øu c√≥
    if args.port:
        port = args.port
    
    # L·∫•y IP address
    hostname = socket.gethostname()
    local_ip = socket.gethostbyname(hostname)
    
    # C·∫≠p nh·∫≠t th√¥ng tin server cho terminal UI (ch·ªâ khi kh√¥ng ·ªü background mode)
    if not BACKGROUND_MODE:
        terminal_ui.update_server_info(SERVER_MODE, port)
    
    # Startup logs
    logger.info(f"Success! HTTP server listening on port {port}")
    logger.warning("Server accepting connections from the local network. Only use this if you know what you are doing!")
    logger.info("")
    
    if SERVER_MODE == "lmstudio":
        logger.info("LM Studio Mode - Supported endpoints:")
        logger.info(f"->\tGET  http://{local_ip}:{port}/v1/models")
        logger.info(f"->\tGET  http://{local_ip}:{port}/v1/models/{{model_id}}")
        logger.info(f"->\tPOST http://{local_ip}:{port}/v1/chat/completions")
        logger.info(f"->\tPOST http://{local_ip}:{port}/v1/completions")
        logger.info(f"->\tPOST http://{local_ip}:{port}/v1/embeddings")
    else:  # ollama mode
        logger.info("Ollama Mode - Supported endpoints:")
        logger.info(f"->\tGET  http://{local_ip}:{port}/api/version")
        logger.info(f"->\tGET  http://{local_ip}:{port}/api/tags")
        logger.info(f"->\tGET  http://{local_ip}:{port}/api/ps")
        logger.info(f"->\tPOST http://{local_ip}:{port}/api/show")
        logger.info(f"->\tPOST http://{local_ip}:{port}/api/generate")
        logger.info(f"->\tPOST http://{local_ip}:{port}/api/chat")
    
    logger.info("")
    logger.info("Custom server with Qwen API integration")
    logger.info("Queue system enabled - requests will be queued if server is busy")
    logger.info("Think mode support enabled - <think> and </think> tags for Qwen thinking phase")
    logger.info("Lock timeout: 2 minutes, Request timeout: 60 seconds")
    logger.info("Server started.")
    logger.info("Just-in-time model loading active.")
    
    # Kh·ªüi t·∫°o chat_id khi server b·∫Øt ƒë·∫ßu
    logger.info("Initializing chat session...")
    chat_id = chat_manager.initialize_chat()
    if chat_id:
        logger.info(f"Chat initialized with ID: {chat_id}")
        if not BACKGROUND_MODE:
            terminal_ui.update_chat_id(chat_id)
            terminal_ui.update_parent_id(None)  # Reset parent_id khi kh·ªüi t·∫°o
    else:
        logger.error("Failed to initialize chat")
    
    # B·∫Øt ƒë·∫ßu terminal UI (ch·ªâ khi kh√¥ng ·ªü background mode)
    if not BACKGROUND_MODE:
        terminal_ui.start()
    
    try:
        # C·∫•u h√¨nh Werkzeug ƒë·ªÉ ch·∫•p nh·∫≠n header r·∫•t d√†i
        from werkzeug.serving import WSGIRequestHandler
        WSGIRequestHandler.max_requestline = 2048 * 1024 * 1024  # 2GB
        WSGIRequestHandler.max_header_size = 2048 * 1024 * 1024  # 2GB
        
        # C·∫•u h√¨nh th√™m cho Werkzeug
        import werkzeug
        werkzeug.serving.WSGIRequestHandler.max_requestline = 2048 * 1024 * 1024
        werkzeug.serving.WSGIRequestHandler.max_header_size = 2048 * 1024 * 1024
        
        # T·∫Øt gi·ªõi h·∫°n request size
        werkzeug.serving.WSGIRequestHandler.max_requestline = None
        werkzeug.serving.WSGIRequestHandler.max_header_size = None
        
        # TƒÉng buffer size cho socket
        import socket
        socket.SOMAXCONN = 1024
        
        # TƒÉng buffer size cho request
        import os
        os.environ['FLASK_MAX_CONTENT_LENGTH'] = str(2048 * 1024 * 1024)  # 2GB
        os.environ['FLASK_MAX_CONTENT_LENGTH'] = '0'  # Kh√¥ng gi·ªõi h·∫°n
        
        app.run(host=args.host, port=port, debug=False, threaded=True, processes=1)
    except KeyboardInterrupt:
        print("\nüõë Server stopped by user")
    finally:
        terminal_ui.stop()
